{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4289bb8-df37-4d12-92a1-7ce4f97a354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d10ee3c-0301-4eb1-89be-01c731a73fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sbs_info(file_name,vs_asr=True):\n",
    "    '''\n",
    "    Returns the names of the origins of the transcripts that were compared to create the sbs file,\n",
    "    the interview name, and whether one of the transcripts was produced by an ASR (= vs_asr)\n",
    "    based on the file path name\n",
    "    Expected path name format (if vs_asr): ResultSbs/coraal10_openai-async/*.txt/ATL_se0_ag1_f_01_1.txt\n",
    "    Else: /Users/aheuser/Documents/CORAAL/Condensed/ResultSbs/aa-rev_amber10/DCB_se3_ag4_m_02_5.sbs.txt\n",
    "    '''\n",
    "    asrs = [\"openai-async\",\"revai_v2-async\"]\n",
    "    path_list = Path(file_name).parts\n",
    "    if vs_asr: #have *.txt part if vs_asr\n",
    "        hyp_spl = path_list[-3].split('_')\n",
    "    else:\n",
    "        hyp_spl = path_list[-2].split('_')\n",
    "    if len(hyp_spl) == 3:\n",
    "        ref = hyp_spl[0]\n",
    "        hyp = hyp_spl[1]+\"_\"+hyp_spl[2]\n",
    "    else:\n",
    "        ref,hyp = hyp_spl\n",
    "    if ref[-1].isdigit():\n",
    "        ref = ref[:-2] #Assuming we'll always cut off a number between 10-99\n",
    "    if hyp[-1].isdigit():\n",
    "        hyp = hyp[:-2]\n",
    "    per_ind = path_list[-1].find(\".\")\n",
    "    return [ref,hyp,path_list[-1][:per_ind],vs_asr]\n",
    "\n",
    "def load_sbs(sbs):\n",
    "    '''\n",
    "    Returns pandas dataframes for all four types of error reporting in the sbs txt file\n",
    "    errors is the main df used in the rest of this notebook so we add extra rows for easier downstream processing\n",
    "    '''\n",
    "    sep_inds = []\n",
    "    with open(sbs) as file:\n",
    "        lines = file.readlines()\n",
    "        for line,i in zip(lines,range(len(lines))):\n",
    "            if line[0] == '-':\n",
    "                sep_inds.append(i)\n",
    "    ref_hyp = pd.read_csv(sbs,delimiter='\\t',nrows=sep_inds[0]-1)\n",
    "    ref_hyp.index = ref_hyp.index+2 #because the line value in the errors is consistently off by 2\n",
    "    errors = pd.read_csv(sbs,delimiter='\\t',skiprows=sep_inds[0]+1,nrows=sep_inds[1]-sep_inds[0]-2)\n",
    "    unigrams = pd.read_csv(sbs,delimiter='\\t',skiprows=sep_inds[1]+1,nrows=sep_inds[2]-sep_inds[1]-2)\n",
    "    bigrams = pd.read_csv(sbs,delimiter='\\t',skiprows=sep_inds[2]+1)\n",
    "    dfs = ref_hyp,errors,unigrams,bigrams\n",
    "    for df in dfs:\n",
    "        df.columns = [col.strip() for col in df.columns]\n",
    "    splat = errors.Group.str.split(\"<->\",expand=True)\n",
    "    errors[\"Left\"] = splat[0]\n",
    "    errors[\"Right\"] = splat[1]\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7317c42a-cf6c-48f5-8d1c-322632692ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_remaining_errors(sbs,accounted_dict,vs_asr):\n",
    "    info = extract_sbs_info(sbs,vs_asr)\n",
    "    dfs = load_sbs(sbs)\n",
    "    ref_hyp,errors,unigrams,bigrams = dfs\n",
    "    if sbs not in accounted_dict:\n",
    "        return errors,ref_hyp\n",
    "    else:\n",
    "        return errors.loc[~errors.Line.isin(accounted_dict[sbs])],ref_hyp\n",
    "\n",
    "def compile_remaining_errors(sbs,accounted_dict,vs_asr):\n",
    "    info = extract_sbs_info(sbs,vs_asr)\n",
    "    errors_df,_ = get_remaining_errors(sbs,accounted_dict,vs_asr)\n",
    "    errors_df['ref_transcript'] = info[0]\n",
    "    errors_df['hyp_transcript'] = info[1]\n",
    "    errors_df['interview_name'] = info[2]\n",
    "    errors_df[\"asr_hyp\"] = info[3]\n",
    "    return errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "945df0b4-f0bd-4b3c-81d9-64acb39f735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_accounted_dict(sbs,new_lines,accounted_dict):\n",
    "    if sbs not in accounted_dict:\n",
    "        accounted_dict[sbs] = new_lines\n",
    "    else:\n",
    "        accounted_dict[sbs]|=new_lines\n",
    "    return accounted_dict\n",
    "    \n",
    "def new_results_df(sbs,accounted_dict,test_funcs,diff_type,vs_asr):\n",
    "    '''\n",
    "     Labels and collects different types of errors based on test_funcs (list of functions)\n",
    "     An error can only count as one type because once a test catches it, its index is added to the accounted_dict\n",
    "     These indices are removed before the next test is run\n",
    "     Returns the accounted for errors (errors_df) and indices (accounted_dict, organized by the sbs file names)\n",
    "    '''\n",
    "    #no summary df - we'll compile this later\n",
    "    first = True\n",
    "    info = extract_sbs_info(sbs,vs_asr)\n",
    "    errors,ref_hyp = get_remaining_errors(sbs,accounted_dict,vs_asr)\n",
    "    for func in test_funcs:\n",
    "        if first: \n",
    "            subsec = errors.loc[errors.apply(lambda x: test_funcs[func](x,ref_hyp),axis=1)].copy() \n",
    "        else:\n",
    "            subsec = unaccounted_for_errors.loc[unaccounted_for_errors.apply(lambda x: test_funcs[func](x,ref_hyp),axis=1)].copy() \n",
    "        subsec[\"ref_transcript\"] = info[0]\n",
    "        subsec[\"hyp_transcript\"] = info[1]\n",
    "        subsec[\"asr_hyp\"] = info[3]\n",
    "        subsec[\"interview_name\"] = info[2]\n",
    "        subsec[\"diff_name\"] = func\n",
    "        subsec[\"diff_typ\"] = diff_type\n",
    "        if not first:\n",
    "            errors_df = pd.concat([errors_df,subsec]).reset_index(drop=True)\n",
    "        else:\n",
    "            errors_df = subsec.reset_index(drop=True)\n",
    "            first=False\n",
    "        errors_merge = pd.merge(errors, errors_df, how='outer', on=['Line','Left','Right','Group'],indicator=True)\n",
    "        unaccounted_for_errors = errors_merge.loc[errors_merge[\"_merge\"] == \"left_only\"].iloc[:,:4]\n",
    "    accounted_dict = add_to_accounted_dict(sbs,set(errors_df.Line.values),accounted_dict)\n",
    "    return errors_df,accounted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17739063-8a91-4f16-ad54-6d36f1b7ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_summary_df(errors_df,total_errors_dict):\n",
    "    '''\n",
    "    Compiles summary statistics per ref/hyp transcript combination \n",
    "    and per test groups (e.g. morpho-syntactic, reductions, etc.)\n",
    "    '''\n",
    "    summary_df = pd.DataFrame(columns=['ref_transcript','hyp_transcript','diff_name','diff_type','frequency','percentage'])\n",
    "    ref_hyp_combos = set(zip(errors_df['ref_transcript'],errors_df['hyp_transcript']))\n",
    "    for ref_hyp_combo in ref_hyp_combos:\n",
    "        ref,hyp = ref_hyp_combo\n",
    "        just_that_combo = errors_df.loc[(errors_df['ref_transcript'] == ref) & (errors_df['hyp_transcript'] == hyp)]\n",
    "        total_errors = total_errors_dict[f\"{ref}|{hyp}\"]\n",
    "        for diff_name in just_that_combo.diff_name.unique():\n",
    "            subsec = just_that_combo.loc[just_that_combo['diff_name'] == diff_name]\n",
    "            freq = len(subsec)\n",
    "            diff_type = subsec.diff_typ.unique()[0]\n",
    "            summary_df.loc[len(summary_df)] = [ref,hyp,diff_name,diff_type,freq,freq/total_errors]\n",
    "        for diff_type in errors_df.diff_typ.unique():\n",
    "            subsec = just_that_combo.loc[just_that_combo['diff_typ'] == diff_type]\n",
    "            freq = len(subsec)\n",
    "            diff_type = subsec.diff_typ.unique()[0]\n",
    "            summary_df.loc[len(summary_df)] = [ref,hyp,\"all_combined\",diff_type,freq,freq/total_errors]\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7d592af-18df-4830-b547-d4f1c75316de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on the user manual\n",
    "#Only added some alternatives, though these are never looked for with the functions, only added them for potential future reference\n",
    "\n",
    "#CORAAL filler words\n",
    "fillers = {\"uh-huh\":[\"uh huh\"], \"uh-uh\":[\"uh uh\",\"nuh-uh\"],\"nuh-uh\":[\"nuh uh\",\"nuh-huh\",\"uh-uh\"], \"mm-hm\":[\"mm hm\",\"mm hmm\"], \"mm\":[\"hm\"], \n",
    "           \"mm-mm\":[\"mm mm\"], \"okay\":[\"ok\", \"kay\"],\"mkay\":[\"mm kay\"], \"yep\":[\"yup\"], \"yup\":[\"yep\"], \"nah\":[\"naw\"],\"naw\":[\"nah\"],\"ooh\":[\"oo\"]}\n",
    "filler_no_alts = [\"oh\",\"ayo\",\"hoo\",\"uh\",\"um\",\"ah\"]\n",
    "\n",
    "#added forms for \"going to\" \"have to\" \"used to\" \"until\" \"about\" \"around\" \"talking about\" \"got you\" \"remember\"\n",
    "red_forms = {\"must have\": [\"musta\"], \"should have\": [\"shoulda\"], \"would have\": [\"woulda\"], \"could have\": [\"coulda\"], \"might have\": [\"mighta\"],\n",
    "            \"going to\": [\"gonna\",\"I'm'a\", \"imma\"], \"have to\": [\"halfta\", \"havta\"], \"trying to\": [\"tryna\"], \"supposed to\":[\"sposta\"], \n",
    "             \"fixing to\":[\"finna\"],\"got to\":[\"gotta\"], \"want to\":[\"wanna\"],\"ought to\":[\"oughta\"], \"because\":[\"cause\",\"cuz\"],\n",
    "            \"until\":[\"til\",\"'til\"], \"them\": [\"'em\"], \"let me\":[\"lemme\"], \"what do you\": [\"watchu\",\"watcha\", \"watchya\"], \n",
    "             \"what are you\": [\"whatchu\",\"whatcha\",\"watchya\"], \"got you\": [\"gotcha\",\"gotchya\"], \"around\":[\"'round\"], \"about\":[\"'bout\"],\n",
    "            \"talking about\": [\"talkin' about\", \"talking 'bout\", \"talkin' 'bout\", \"talkin 'bout\", \"talkin' bout\", \"talkin bout\"],\n",
    "            \"remember\":[\"'member\"]}\n",
    "\n",
    "#Need a list of what CORAAL allows - should be easy to make with the info there\n",
    "red_forms_coraal = [\"must have\",\"musta\",\"should have\",\"shoulda\",\"would have\",\"woulda\",\"could have\",\"coulda\",\"might have\",\"mighta\",\n",
    "                    \"going to\",\"gonna\",\"I'm'a\", \"have to\",\"halfta\",\"trying to\",\"tryna\",\"supposed to\", \"sposta\", \"fixing to\", \"finna\", \n",
    "                    \"got to\", \"gotta\", \"want to\", \"wanna\", \"ought to\", \"oughta\", \"because\", \"cause\", \"until\", \"til\", \"about\", \"remember\",\n",
    "                    \"around\", \"talking about\", \"them\", \"'em\", \"let me\", \"lemme\", \"what do you\", \"whatchu\", \"what are you\", \"whatcha\",\n",
    "                    \"got you\", \"gotcha\"]\n",
    "\n",
    "coraal_lex = {\"aight\":\"alright\",\"aks\":\"ask\", \"'bacca\":\"tobacco\", \"bih\":\"bitch\", \"brazy\":\"crazy\", \"bruh\":\"bro\", \"cuz\":\"cousin\",\"'em\":\"them\",\n",
    "              \"fella\":\"fellow\", \"hisself\":\"himself\", \"mama\":\"momma\", \"turnt\":\"turned\", \"and them\": [\"nem\",\"dem\"], \"youngin\": \"youngen\"} \n",
    "#More for \"and them\" as shown in function\n",
    "coraal_lex_no_alts = [\"ay\", \"bougie\", \"go go\", \"jai\", \"effed up\", \"mumbo sauce\", \"murk\", \"racthet\", \"shorty\", \"wilding\", \n",
    "                      \"wont\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b4f6300-3fc4-4b49-b7c0-3a56bdf7d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_row(row):\n",
    "    left = row[\"Left\"].strip()\n",
    "    right = row[\"Right\"].strip()\n",
    "    return left,right\n",
    "\n",
    "def no_chars(phrase):\n",
    "    #Helper function to check that a string just has numbers and number-relevant punctuation\n",
    "    for char in phrase:\n",
    "        if not str.isdigit(char) and not char in \",.-\":\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def written_numbers(row):\n",
    "    #Check if the error involves one of the lines just being numbers\n",
    "    left,right = check_row(row)\n",
    "    if no_chars(left) or no_chars(right):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_coraal_filler(row):\n",
    "    #Check if the error involves a CORAAL-defined filler\n",
    "     #Checking right side as well for cross comparison - even though CORAAL will likely always be on the left\n",
    "    left,right = check_row(row)\n",
    "    if left in fillers or left in filler_no_alts or right in fillers or right in filler_no_alts:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#Covered by is_coraal_filler\n",
    "def del_filler_coraal(row):\n",
    "    #Check if a word defined as a filler by CORAAL is deleted in the other transcript\n",
    "    left,right = check_row(row)\n",
    "    if right == \"***\":\n",
    "        if is_coraal_filler(left):\n",
    "            return True\n",
    "    if left == \"***\":\n",
    "        if is_coraal_filler(right):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def del_restart(row,ref_hyp):\n",
    "    #Check if a line ending with a restart was deleted in either transcript\n",
    "    left,right = check_row(row)\n",
    "    if right == \"***\":\n",
    "        if left[-1] == \"-\":\n",
    "            return True\n",
    "    if left == \"***\":\n",
    "        if right[-1] == \"-\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_coraal_red_form(row):\n",
    "    #Checks whether a word that may be reduced and then transcribed differently in the CORAAL transcript is involved in the error\n",
    "    #Note that the error may not have to do with transcriotion of the reduced vs full form, the error may be unrelated\n",
    "    left,right = check_row(row)\n",
    "    if left in red_forms_coraal or right in red_forms_coraal:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_restart(row,ref_hyp):\n",
    "    #restarts - \"[word]-\" vs \"word\"\n",
    "    #Checks whether the error is the result of one of the lines being marked as a restart by having a trailing dash\n",
    "    left,right = check_row(row)\n",
    "    if left+\"-\" == right or left==right+\"-\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_coraal_lex(row):\n",
    "    #Checks whether the error involves one of the words being defined as a dialect specific lexical item by CORAAL\n",
    "    left,right = check_row(row)\n",
    "    if left in coraal_lex or left in coraal_lex_no_alts or right in coraal_lex or right in coraal_lex_no_alts:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c6ce5f-10cf-410f-9e7c-9465314923aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fillers = []\n",
    "for key in fillers:\n",
    "    all_fillers.append(key)\n",
    "    all_fillers+=fillers[key]\n",
    "all_fillers+=filler_no_alts\n",
    "\n",
    "def filler_sub(row,ref_hype):\n",
    "    left,right = check_row(row)\n",
    "    if left in all_fillers and right in all_fillers:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def filler_del(row,ref_hype):\n",
    "    left,right = check_row(row)\n",
    "    if (left in all_fillers and right == \"***\") or (left == \"***\" and right in all_fillers):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ad2f5c8-42ab-43cd-a087-d82c0b40c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_rep(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    if left == \"***\":\n",
    "        if row.Line > 2:\n",
    "            if ref_hyp.loc[row.Line-1][\"ref_token\"] == right and ref_hyp.loc[row.Line-1][\"hyp_token\"] == right:\n",
    "                return True\n",
    "        if row.Line < len(ref_hyp)+1:\n",
    "            if ref_hyp.loc[row.Line+1][\"ref_token\"] == right and ref_hyp.loc[row.Line+1][\"hyp_token\"] == right:\n",
    "                return True\n",
    "    if right == \"***\":\n",
    "        if row.Line > 2:\n",
    "            if ref_hyp.loc[row.Line-1][\"hyp_token\"] == left and ref_hyp.loc[row.Line-1][\"ref_token\"] == left:\n",
    "                return True\n",
    "        if row.Line < len(ref_hyp)+1:\n",
    "            if ref_hyp.loc[row.Line+1][\"hyp_token\"] == left and ref_hyp.loc[row.Line+1][\"ref_token\"] == left:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8058a5f6-115a-4549-9b62-08dff3fbc96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_red_forms = []\n",
    "for key in red_forms:\n",
    "    all_red_forms.append(key)\n",
    "    all_red_forms+=red_forms[key]\n",
    "\n",
    "def is_red_form(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    if left in all_red_forms or right in all_red_forms:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def x_reduction(left_words,right_words,x,modals):\n",
    "    contr_len = len(x)\n",
    "    if len(left_words) > len(right_words):\n",
    "        if left_words[-1] in modals and right_words[-1][-(contr_len+1):] == f\"'{x}\":\n",
    "            return True\n",
    "    elif len(left_words) < len(right_words):\n",
    "        if right_words[-1] in modals and left_words[-1][-(contr_len+1):] == f\"'{x}\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "d_reduction = lambda left,right: x_reduction(left,right,'d',[\"did\",\"had\",\"would\"])\n",
    "s_reduction = lambda left,right: x_reduction(left,right,'s',[\"is\",\"has\",\"us\"])\n",
    "ve_reduction = lambda left,right: x_reduction(left,right,'ve',[\"have\"])\n",
    "ll_reduction = lambda left,right: x_reduction(left,right,'ll',[\"will\",\"shall\"])\n",
    "re_reduction = lambda left,right: x_reduction(left,right,'re',[\"are\"])\n",
    "t_reduction = lambda left,right: x_reduction(left,right,'t',[\"not\"])\n",
    "\n",
    "def is_contraction(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    left_words = left.split()\n",
    "    right_words = right.split()\n",
    "    contr_funcs = [d_reduction,s_reduction,ve_reduction,ll_reduction,re_reduction,t_reduction]\n",
    "    for test in contr_funcs:\n",
    "        if test(left_words,right_words):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "all_lex_forms = []\n",
    "for key in coraal_lex:\n",
    "    all_lex_forms.append(key)\n",
    "    all_lex_forms+=coraal_lex[key]\n",
    "all_lex_forms+=coraal_lex_no_alts\n",
    "\n",
    "def is_lexical(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    if left in all_lex_forms or right in all_lex_forms:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c40f99e8-6ed5-40cd-b9e9-60c10c59b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbatim = {\"filler_sub\":filler_sub, \"filler_del\":filler_del,\"is_restart\":is_restart, \"del_restart\":del_restart, \"del_rep\":del_rep}\n",
    "reductions = {\"is_red_form\":is_red_form, \"is_contraction\":is_contraction}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a4688d7-2fe7-400e-88c0-c1f85822455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Morphosyntactic difference functions\n",
    "def dropped_apos_s(row):\n",
    "    left,right = check_row(row)\n",
    "    if right == f\"{left}'s\" or left == f\"{right}'s\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#Reformulation based on Rickford 1999\n",
    "def drop_final_cons_1_2_4(row): #should check based on IPA transcription because no and now and a and an are not good examples\n",
    "    left,right = check_row(row)\n",
    "    if len(left) > len(right) and left[-1] != 's':\n",
    "        if left[:-1] == right:\n",
    "            return True\n",
    "    if right[-1] == \"'\" and left[-1] != 's':\n",
    "        if left[:-1] == right[:-1]:\n",
    "            return True\n",
    "    if len(right) > len(left) and right[-1] != 's':\n",
    "        if right[:-1] == left:\n",
    "            return True\n",
    "    if left[-1] == \"'\" and right[-1] != 's':\n",
    "        if right[:-1] == left[:-1]:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "#Probably also matching possessive s dropping\n",
    "def cop_aux_del_19a(row,ref_hyp):\n",
    "    if dropped_apos_s(row):\n",
    "        return True\n",
    "    left,right = check_row(row)\n",
    "    #will also cover some deletions\n",
    "    if left == \"***\":\n",
    "        if right in [\"is\",\"are\"]:\n",
    "            return True\n",
    "    if right == \"***\": #need to check the other side? Depends on whether we consider one gold standard\n",
    "        if left in [\"is\",\"are\"]:\n",
    "            return True\n",
    "    return False \n",
    "\n",
    "#One \"do\" deletion doesn't seem to be invariant \"be\": \"you come home and you got your homework do what you have-\"\n",
    "#vs \"you come home and you got your homework what y'all\"\n",
    "#Can't check for \"she'd be\" vs \"she be\" because t\n",
    "def invariant_be_19b(row,ref_hyp):\n",
    "    #E.g. \"She don't be sick, do she?\" vs \"She isn't sick, is she?\" depends on alignment\n",
    "    #\"don't be\" for \"isn't\" \n",
    "    bes = [\"be\", \"do\", \"don't\", \"don't be\", \"do be\"]\n",
    "    se_conj = [\"is\",\"are\", \"isn't\",\"aren't\"] \n",
    "    left,right = check_row(row)\n",
    "    if left in bes and right in se_conj or right in bes and left in se_conj:\n",
    "        return True\n",
    "    if left in bes and right == \"***\" or right in bes and left == \"***\":\n",
    "        return True\n",
    "    #19.c3 from Spears 2017\n",
    "    try:\n",
    "        if ref_hyp.loc[row.Line+1][\"ref_token\"] == \"be\":\n",
    "            if left == right+\"'d\" or right == left+\"'d\":\n",
    "                return True\n",
    "            if left == \"would\" and right == \"***\" or left == \"***\" and right == \"would\":\n",
    "                return True\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def future_ll_del_19c(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    if left+\"'ll\" == right or right+\"'ll\" == left:\n",
    "        return True\n",
    "    #Also covering deletion of will\n",
    "    if left == \"will\" and right == \"***\" or left == \"***\" and right == \"will\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#No reason a transcription would delete the word steady so won't look for 19d\n",
    "#Stress isn't orthographically transcribed so skipping 19e too\n",
    "\n",
    "#Got 0 examples which might be right but need to check that there really are none\n",
    "def pre_been_insertion_19ef(row,ref_hyp): #need to check this, will correct ref_hyp be available? Does line need to be converted to int?\n",
    "    line = row[\"Line\"]\n",
    "    left,right = check_row(row)\n",
    "    try:\n",
    "        if ref_hyp.loc[line+1][\"ref_token\"] == \"been\": #hyp_token will be the same\n",
    "            if left == \"***\" and right == \"has\" or left == \"has\" and right == \"***\":\n",
    "                return True\n",
    "        return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def done_del_19g(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    if (left == \"done\" and right == \"***\") or (left == \"***\" and right == \"done\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#\"She be done had her baby\" for \"She will have had her baby\" - probably would not go so far from the actual signal\n",
    "#Possible corrections: \n",
    "#\"She had her baby\" - deletion of \"be done\" need to check for this, \"She's had her baby\" seems further from the signal and unlikely\n",
    "#\"She done had her baby\" or \"She's done had her baby\" - check for these with 19b function\n",
    "#\"She be had her baby\" - weird and doubt anyone would transcribe it like this but caught with 19g function\n",
    "\n",
    "#Got 0 examples so need to search to make sure that's accurate\n",
    "#Added to this to cover Spears 2017 examples for 19.h3\n",
    "def be_done_del_19h(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    if (left == \"be done\" and right == \"***\") or (left == \"***\" and right == \"be done\"):\n",
    "        return True\n",
    "    #check number of words in the substituions\n",
    "    left_words = left.split()\n",
    "    right_words = right.split()\n",
    "    #this might catch errors beyond this, but not easily\n",
    "    if len(left_words) > len(right_words):\n",
    "        if left_words[-2:] == [\"be\", \"done\"] or left_words[:2] == [\"be\", \"done\"]:\n",
    "            return True\n",
    "    else:\n",
    "        if right_words[-2:] == [\"be\", \"done\"] or right_words[:2] == [\"be\", \"done\"]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "#\"He finna go\" -> \"He's gonna go\" most likely?\n",
    "#Deletion of finna seems more likely in plural case e.g. \"They finna go\" vs \"They go\" but \"They're gonna go\" seems more likely\n",
    "\n",
    "#Got 0 examples - this is accurate based on CORAAL search, need to check with a sbs file that has it\n",
    "def finna_19i(row,ref_hyp): #don't need to worry about how it was corrected for\n",
    "    left,right = check_row(row)\n",
    "    if \"finna\" in left and not \"finna\" in right:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#Got 0 examples - this is accurate based on CORAAL search, need to check with a sbs file that has it\n",
    "def double_modals_19l(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    left_words = left.split()\n",
    "    right_words = right.split()\n",
    "    first_modal = False\n",
    "    modals1 = [\"may\",\"might\",\"must\"]\n",
    "    modals2 = [\"can\",\"could\",\"don't\"]\n",
    "    one_double_modal = False\n",
    "    word_lists = [left_words, right_words]\n",
    "    for word_list in word_lists:\n",
    "        for word in word_list:\n",
    "             if word in modals1:\n",
    "                 first_modal = True\n",
    "             elif first_modal:\n",
    "                if word in modals2:\n",
    "                    one_double_modal = !one_double_modal\n",
    "                first_modal = False\n",
    "        first_modal = False\n",
    "    if one_double_modal:\n",
    "        return True\n",
    "    try:\n",
    "        if ref_hyp.loc[row.Line-1][\"ref_token\"] in modals1:\n",
    "            if left == \"***\" and right in modals2 or left in modals2 and right == \"***\":\n",
    "                return True\n",
    "    except: pass\n",
    "    try:\n",
    "        if ref_hyp.loc[row.Line+1][\"ref_token\"] in modals2:\n",
    "            if left == \"***\" and right in modals1 or left in modals1 and right == \"***\":\n",
    "                return True\n",
    "    except: pass\n",
    "    return False\n",
    "\n",
    "#Simple substitution e.g. run vs runs, no way to verify the word's a verb without POS tagging so it'll catch more than verbs\n",
    "def final_s_del_20a_21b(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    if left+\"s\" == right or left == right+\"s\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#Look specifically for \"have\" and \"don't\"??\n",
    "def switch_sin_plu_20b(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    if (left == \"is\" and right == \"are\") or (left == \"are\" and right == \"is\"):\n",
    "        return True\n",
    "    if (left == \"was\" and right == \"were\") or (left == \"were\" and right == \"was\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#Only counting regular verbs, even though irregular verbs are very frequent\n",
    "#This will cacth 20f too and be unable to distinguish them\n",
    "def stem_for_past_20e(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    if left+\"ed\" == right or left == right+\"ed\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#def lack_plural_noun_21b(row):\n",
    "    #return dropped_s(row)\n",
    "\n",
    "#21c, many possible spelling variations, e.g. \"an' 'em,\" \"(')?nem,\" \"an' them,\" \"and them\" \n",
    "#Don't expect to see this much if at all - found 0 examples in the file with the CORAAL search as well\n",
    "def assos_plural_21c(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    variations = [\"an' 'em\", \"an em\", \"an' em\", \"an 'em\", \"nem\" \"'nem\", \"and them\", \"and 'em\", \"an' them\"]\n",
    "    if left in variations and right in variations:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#May not be actual examples of this - not sure what the right surrounding sentence structure should be/how to search for it\n",
    "def add_pronoun_21d(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    pronouns = [\"she\",\"he\",\"it\", \"they\"] #all third person because others can't have other referents\n",
    "    if (left in pronouns and right == \"***\") or (left == \"***\" and right in pronouns):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def yall_they_poss_21e(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    yall_vers = [\"y'all\", \"yall\", \"you all\"]\n",
    "    if (left in yall_vers and right == \"your\") or (left == \"your\" and right in yall_vers):\n",
    "        return True\n",
    "    if (left == \"they\" and right == \"their\") or (left == \"their\" and right == \"they\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def drop_rel_pro_21g(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    rel_pros = [\"who\",\"which\",\"what\",\"that\"]\n",
    "    if left in rel_pros and right == \"***\" or left == \"***\" and right in rel_pros:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def aint_sub_22a(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    neg_mod = [\"am not\", \"isn't\", \"is not\", \"aren't\", \"are not\", \"hasn't\", \"has not\", \"haven't\", \"have not\", \"did not\", \"didn't\"]\n",
    "    aint_vars = [\"ain't\", \"ain'\", \"aint\", \"ain\"]\n",
    "    if left in neg_mod and right in aint_vars or left in aint_vars and right in neg_mod:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def mult_neg_22b(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    neg_indef = [\"nothing\", \"nobody\", \"no one\", \"nowhere\"]\n",
    "    alt_indef = [\"anything\", \"anybody\", \"any one\", \"anywhere\"]\n",
    "    if left in neg_indef and right in alt_indef or left in alt_indef and right in neg_indef:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#Found 0 examples of this - need to check with an sbs file that has this/create an artificial example\n",
    "def if_whether_del_23b(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    if_whether = [\"if\", \"whether\"]\n",
    "    if left in if_whether and right == \"***\" or left == \"***\" and right in if_whether:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def it_there_sub_24a(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    it_var = [\"it's\", \"it is\", \"it was\", \"ain't\", \"ain'\", \"ain\", \"isn't\", \"is not\"]\n",
    "    there_var = [\"there's\", \"there is\", \"there was\" \"there isn't\", \"there ain't\", \"there is not\", \"there ain'\", \"there aint\"]\n",
    "    if left in it_var and right in there_var or left in there_var and right in it_var:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#Found 0 examples of this - need to check with an sbs file that has this/create an artificial example\n",
    "def they_got_there_are_sub_24b(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    if left == \"they got\" and right == \"there are\" or left == \"there are\" and right == \"they got\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#No longer captures generic go deletion - gets 0 matches which is consistent with CORAAL search \n",
    "#Need to check on other sbs file or make an artificial example\n",
    "def here_go_24c(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    mods = [\"is\", \"are\"] #always assuming positive for presentational\n",
    "    if left == \"go\" and right in mods or left in mods and right == \"go\":\n",
    "        return True\n",
    "    if left == \"here's\" and right == \"here go\" or left == \"here go\" and right == \"here's\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#need to add nominative for possessive substitution corresponding to part of 8\n",
    "def they_their_sub_8ish(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    if left == \"they\" and right == \"their\" or left == \"their\" and right == \"they\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def gone_go_19n(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    if left in [\"gone\", \"go\"]:\n",
    "        if right in [\"gonna\",\"***\"]:\n",
    "            return True\n",
    "    if right in [\"gone\", \"go\"]:\n",
    "        if left in [\"gonna\",\"***\"]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def gone_come_19o(row,ref_hyp):\n",
    "    left,right = check_row(row)\n",
    "    left_words = left.split()\n",
    "    right_words = right.split()\n",
    "    if left_words[-2:] == [\"gone\",\"come\"]:\n",
    "        return True\n",
    "    if right_words[-2:] == [\"gone\",\"come\"]:\n",
    "        return True\n",
    "    try:\n",
    "        if ref_hyp.loc[row.Line-1][\"ref_token\"] == \"gone\":\n",
    "            if left == \"come\" and right == \"***\" or left == \"***\" and right == \"come\":\n",
    "                return True\n",
    "    except: pass\n",
    "    try:\n",
    "        if ref_hyp.loc[row.Line+1][\"ref_token\"] == \"come\":\n",
    "            if left == \"gone\" and right == \"***\" or left == \"***\" and right == \"gone\":\n",
    "                return True\n",
    "    except: pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68b3c052-76a1-4424-b984-c84043ed7476",
   "metadata": {},
   "outputs": [],
   "source": [
    "morpho_syntax = {\"cop_aux_del_19a\":cop_aux_del_19a, \"invariant_be_19b\":invariant_be_19b, \"future_ll_del_19c\":future_ll_del_19c,\n",
    "\"pre_been_insertion_19ef\":pre_been_insertion_19ef, \"done_del_19g\":done_del_19g, \"be_done_del_19h\": be_done_del_19h, \"finna_19i\": finna_19i,\n",
    "\"double_modals_19l\":double_modals_19l,\"final_s_del_20a_21b\": final_s_del_20a_21b, \"switch_sin_plu_20b\": switch_sin_plu_20b,\n",
    "\"stem_for_past_20e\": stem_for_past_20e, \"assos_plural_21c\": assos_plural_21c,\"add_pronoun_21d\": add_pronoun_21d, \n",
    "\"aint_sub_22a\":aint_sub_22a,\"if_whether_del_23b\": if_whether_del_23b,\"it_there_sub_24a\": it_there_sub_24a, \n",
    "\"they_got_there_are_sub_24b\": they_got_there_are_sub_24b, \"here_go_24c\": here_go_24c, \"they_their_sub_8ish\": they_their_sub_8ish,\n",
    "                 \"gone_go_19n\":gone_go_19n, \"gone_come_19o\":gone_come_19o, \"is_red_form\":is_red_form,\"is_lexical\":is_lexical}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f7890b4-0675-4eb0-8c1b-54eb5a10e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_buckets = {\"morpho_syntax\":morpho_syntax,\"reductions\":reductions,\"verbatim\":verbatim}\n",
    "just_verbatim = {\"verbatim\":verbatim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6eb7a4b-54c2-4a58-993d-8d313fc84824",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs_dir = \"/Users/aheuser/Documents/CORAAL/Condensed/ResultSbs\"\n",
    "human_dirs = [\"aa-rev_amber10\", \"aa-rev_coraal10\", \"aa-rev_rev10\", \"coraal_amber10\", \"rev_amber10\", \"rev_coraal10\"]\n",
    "asr_dirs = [\"aa-rev10_\", \"coraal10_\", \"rev10_\", \"amber_\",\"openai-async_\"]\n",
    "asrs = [\"openai-async\",\"revai_v2-async\"]\n",
    "total_errors_dict = {}\n",
    "for dir in human_dirs:\n",
    "    path = Path(sbs_dir+\"/\"+dir)\n",
    "    for file in path.glob('*.*'):\n",
    "        if file.is_file():\n",
    "            info = extract_sbs_info(file,False)\n",
    "            dfs = load_sbs(file)\n",
    "            ref_hyp,errors,unigrams,bigrams = dfs\n",
    "            ref_hyp_string = f\"{info[0]}|{info[1]}\"\n",
    "            if ref_hyp_string not in total_errors_dict:\n",
    "                total_errors_dict[ref_hyp_string] = len(errors)\n",
    "            else:\n",
    "                total_errors_dict[ref_hyp_string]+=len(errors)\n",
    "for dir in asr_dirs:\n",
    "    for i in range(2):\n",
    "        path = Path(sbs_dir+\"/\"+dir+asrs[i]+\"/*.txt\")\n",
    "        for file in path.glob('*.*'):\n",
    "            if file.is_file():\n",
    "                info = extract_sbs_info(file,True)\n",
    "                dfs = load_sbs(file)\n",
    "                ref_hyp,errors,unigrams,bigrams = dfs\n",
    "                ref_hyp_string = f\"{info[0]}|{info[1]}\"\n",
    "                if ref_hyp_string not in total_errors_dict:\n",
    "                    total_errors_dict[ref_hyp_string] = len(errors)\n",
    "                else:\n",
    "                    total_errors_dict[ref_hyp_string]+=len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3067d476-670c-4e68-89cd-6969877d3d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aa-rev|amber': 11173,\n",
       " 'aa-rev|coraal': 8226,\n",
       " 'aa-rev|rev': 7448,\n",
       " 'coraal|amber': 12035,\n",
       " 'rev|amber': 11169,\n",
       " 'rev|coraal': 8747,\n",
       " 'aa-rev|openai-async': 9484,\n",
       " 'aa-rev|revai_v2-async': 9484,\n",
       " 'coraal|openai-async': 9994,\n",
       " 'coraal|revai_v2-async': 7797,\n",
       " 'rev|openai-async': 9454,\n",
       " 'rev|revai_v2-async': 7049,\n",
       " 'amber|openai-async': 10073,\n",
       " 'amber|revai_v2-async': 9519,\n",
       " 'openai-async|revai_v2-async': 8330}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_errors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27978368-9941-4696-8103-9e27f9bfaf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounted_dict = {}\n",
    "first = True\n",
    "for dir in human_dirs:\n",
    "    path = Path(sbs_dir+\"/\"+dir)\n",
    "    for file in path.glob('*.*'):\n",
    "        for bucket in all_buckets:\n",
    "            if file.is_file():\n",
    "                errors,accounted_dict = new_results_df(file,accounted_dict,all_buckets[bucket],bucket,False)\n",
    "                if first: \n",
    "                    errors_df = errors.copy()\n",
    "                    first = False\n",
    "                else:\n",
    "                    errors_df = pd.concat([errors_df,errors]).reset_index(drop=True)\n",
    "for dir in asr_dirs:\n",
    "    for i in range(2):\n",
    "        path = Path(sbs_dir+\"/\"+dir+asrs[i]+\"/*.txt\")\n",
    "        for file in path.glob('*.*'):\n",
    "            for bucket in all_buckets:\n",
    "                if file.is_file():\n",
    "                    errors,accounted_dict = new_results_df(file,accounted_dict,all_buckets[bucket],bucket,True)\n",
    "                    errors_df = pd.concat([errors_df,errors]).reset_index(drop=True)              \n",
    "\n",
    "#Commented out the code with the original final name, current code is for testing \n",
    "#errors_df.to_csv(\"/Users/aheuser/Documents/CORAAL/Condensed/ResultSbs/bucket_errors_new.csv\")\n",
    "errors_df.to_csv(\"/Users/aheuser/Documents/CORAAL/Condensed/ResultSbs/bucket_errors_new_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3898e0e4-b01e-4eeb-a6ce-f77543a91800",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = build_summary_df(errors_df,total_errors_dict)\n",
    "#summary_df.to_csv(\"/Users/aheuser/Documents/CORAAL/Condensed/ResultSbs/bucket_summary_new.csv\")\n",
    "summary_df.to_csv(\"/Users/aheuser/Documents/CORAAL/Condensed/ResultSbs/bucket_summary_new_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
